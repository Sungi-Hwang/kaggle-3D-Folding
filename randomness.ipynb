{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87793,"databundleVersionId":12024591,"sourceType":"competition"},{"sourceId":10855324,"sourceType":"datasetVersion","datasetId":6742586},{"sourceId":11118830,"sourceType":"datasetVersion","datasetId":6933267},{"sourceId":224830487,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"MODEL_TYPE='protenix'\nVALIDATION=False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:10.051549Z","iopub.execute_input":"2025-05-09T03:31:10.051861Z","iopub.status.idle":"2025-05-09T03:31:10.055729Z","shell.execute_reply.started":"2025-05-09T03:31:10.051837Z","shell.execute_reply":"2025-05-09T03:31:10.054733Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Install requirements ","metadata":{}},{"cell_type":"code","source":"import sys, torch, subprocess, os, platform, re, json, zipfile, pathlib\nprint(\"Python  :\", sys.version.split()[0])      # 예: 3.10.12\nprint(\"PyTorch :\", torch.__version__)           # 예: 2.1.0+cu118\nprint(\"CUDA    :\", torch.version.cuda)          # 예: 11.8\n!nvidia-smi | grep -m1 \"Driver Version\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T05:13:14.938838Z","iopub.execute_input":"2025-05-09T05:13:14.939206Z","iopub.status.idle":"2025-05-09T05:13:18.735686Z","shell.execute_reply.started":"2025-05-09T05:13:14.939153Z","shell.execute_reply":"2025-05-09T05:13:18.734511Z"}},"outputs":[{"name":"stdout","text":"Python  : 3.10.12\nPyTorch : 2.5.1+cu121\nCUDA    : 12.1\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"RNA 3D 예측 결과와 실제 구조를 비교하고, 이를 평가하는 파이프 라인.","metadata":{}},{"cell_type":"code","source":"if MODEL_TYPE=='protenix' and VALIDATION:\n    !pip install --no-deps protenix\n    !pip install biopython\n    !pip install ml-collections\n    !pip install biotite==1.0.1\n    !pip install rdkit\n!export PROTENIX_DATA_ROOT_DIR=/kaggle/input/protenix-checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:10.056846Z","iopub.execute_input":"2025-05-09T03:31:10.057151Z","iopub.status.idle":"2025-05-09T03:31:24.674628Z","shell.execute_reply.started":"2025-05-09T03:31:10.057118Z","shell.execute_reply":"2025-05-09T03:31:24.673672Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: protenix in /usr/local/lib/python3.10/dist-packages (0.4.6)\nRequirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.85)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: ml-collections in /usr/local/lib/python3.10/dist-packages (1.1.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.2)\nRequirement already satisfied: biotite==1.0.1 in /usr/local/lib/python3.10/dist-packages (1.0.1)\nRequirement already satisfied: biotraj<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from biotite==1.0.1) (1.2.2)\nRequirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from biotite==1.0.1) (1.1.0)\nRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from biotite==1.0.1) (3.4.2)\nRequirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.10/dist-packages (from biotite==1.0.1) (1.26.4)\nRequirement already satisfied: requests>=2.12 in /usr/local/lib/python3.10/dist-packages (from biotite==1.0.1) (2.32.3)\nRequirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.10/dist-packages (from biotraj<2.0,>=1.0->biotite==1.0.1) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.25->biotite==1.0.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.25->biotite==1.0.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.25->biotite==1.0.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.25->biotite==1.0.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.25->biotite==1.0.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.25->biotite==1.0.1) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==1.0.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==1.0.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==1.0.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite==1.0.1) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.25->biotite==1.0.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.25->biotite==1.0.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.25->biotite==1.0.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.25->biotite==1.0.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.25->biotite==1.0.1) (2024.2.0)\nRequirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2024.9.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rdkit) (2024.2.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"! mkdir /af3-dev \n! ln -s /kaggle/input/protenix-checkpoints /af3-dev/release_data\n! ls /af3-dev/release_data/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:24.676285Z","iopub.execute_input":"2025-05-09T03:31:24.676549Z","iopub.status.idle":"2025-05-09T03:31:25.033609Z","shell.execute_reply.started":"2025-05-09T03:31:24.676524Z","shell.execute_reply":"2025-05-09T03:31:25.032843Z"}},"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘/af3-dev’: File exists\nln: failed to create symbolic link '/af3-dev/release_data/protenix-checkpoints': Read-only file system\ncomponents.v20240608.cif\t\tmodel_v0.2.0.pt\ncomponents.v20240608.cif.rdkit_mol.pkl\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Helper scripts","metadata":{}},{"cell_type":"code","source":"import Bio\n\nfrom copy import deepcopy\n\nimport pandas as pd\nfrom Bio.PDB import Atom, Model, Chain, Residue, Structure, PDBParser\nfrom Bio import SeqIO\nimport os, sys\nimport re\nimport numpy as np\nimport torch\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\ntime0=time.time()\n\nprint('IMPORT OK !!!!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:25.035395Z","iopub.execute_input":"2025-05-09T03:31:25.035619Z","iopub.status.idle":"2025-05-09T03:31:25.041275Z","shell.execute_reply.started":"2025-05-09T03:31:25.035600Z","shell.execute_reply":"2025-05-09T03:31:25.040452Z"}},"outputs":[{"name":"stdout","text":"IMPORT OK !!!!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"PYTHON = sys.executable\nprint('PYTHON',PYTHON)\n\nRHONET_DIR=\\\n'/kaggle/input/data-for-demo-for-rhofold-plus-with-kaggle-msa/RhoFold-main'\n#'<your downloaded rhofold repo>/RhoFold-main'\n\nUSALIGN = \\\n'/kaggle/working//USalign'\n#'<your us align path>/USalign'\n\nos.system('cp /kaggle/input/usalign/USalign /kaggle/working/')\nos.system('sudo chmod u+x /kaggle/working//USalign')\nsys.path.append(RHONET_DIR)\n\n\nDATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\n\n\n# helper ----\n# 딕셔너리 <-> 키 매핑\nclass dotdict(dict):\n\t__setattr__ = dict.__setitem__\n\t__delattr__ = dict.__delitem__\n\n\tdef __getattr__(self, name):\n\t\ttry:\n\t\t\treturn self[name]\n\t\texcept KeyError:\n\t\t\traise AttributeError(name)\n\n# visualisation helper ----\ndef set_aspect_equal(ax):\n\tx_limits = ax.get_xlim()\n\ty_limits = ax.get_ylim()\n\tz_limits = ax.get_zlim()\n\n\t# Compute the mean of each axis\n\tx_middle = np.mean(x_limits)\n\ty_middle = np.mean(y_limits)\n\tz_middle = np.mean(z_limits)\n\n\t# Compute the max range across all axes\n\tmax_range = max(x_limits[1] - x_limits[0],\n\t\t\t\t\ty_limits[1] - y_limits[0],\n\t\t\t\t\tz_limits[1] - z_limits[0]) / 2.0\n\n\t# Set the new limits to ensure equal scaling\n\tax.set_xlim(x_middle - max_range, x_middle + max_range)\n\tax.set_ylim(y_middle - max_range, y_middle + max_range)\n\tax.set_zlim(z_middle - max_range, z_middle + max_range)\n\n\n\n\n# xyz df helper --------------------\ndef get_truth_df(target_id):\n    truth_df = LABEL_DF[LABEL_DF['target_id'] == target_id]\n    truth_df = truth_df.reset_index(drop=True)\n    return truth_df\n\ndef parse_output_to_df(output, seq, target_id):\n    df = []\n    chain_data = []\n    for i, res in enumerate(seq):\n        d=dict(ID = target_id,\n                    resname=res,\n                    resid=i+1)\n        for n in range(len(output)):\n            d={**d, f'x_{n+1}': round(output[n,i,0].item(),3),\n                     f'y_{n+1}': round(output[n,i,1].item(),3),\n                     f'z_{n+1}': round(output[n,i,2].item(),3)}\n        chain_data.append(d)\n\n    if len(chain_data)!=0:\n        chain_df = pd.DataFrame(chain_data)\n        df.append(chain_df)\n        ##print(chain_df)\n    return df\n\ndef parse_pdb_to_df(pdb_file, target_id):\n    parser = PDBParser()\n    structure = parser.get_structure('', pdb_file)\n\n    df = []\n    for model in structure:\n        for chain in model:\n            print(chain)\n            chain_data = []\n            for residue in chain:\n                # print(residue)\n                if residue.get_resname() in ['A', 'U', 'G', 'C']:\n                    # Check if the residue has a C1' atom\n                    if 'C1\\'' in residue:\n                        atom = residue['C1\\'']\n                        xyz = atom.get_coord()\n                        resname = residue.get_resname()\n                        resid = residue.get_id()[1]\n\n                        #todo detect discontinous: resid = prev_resid+1\n                        #ID\tresname\tresid\tx_1\ty_1\tz_1\n                        chain_data.append(dict(\n                            ID = target_id+'_'+str(resid),\n                            resname=resname,\n                            resid=resid,\n                            x_1=xyz[0],\n                            y_1=xyz[1],\n                            z_1=xyz[2],\n                        ))\n                        ##print(f\"Residue {resname} {resid}, Atom: {atom.get_name()}, xyz: {xyz}\")\n\n            if len(chain_data)!=0:\n                chain_df = pd.DataFrame(chain_data)\n                df.append(chain_df)\n                ##print(chain_df)\n    return df\n\n# usalign helper --------------------\ndef write_target_line(\n    atom_name, atom_serial, residue_name, chain_id, residue_num, x_coord, y_coord, z_coord, occupancy=1.0, b_factor=0.0, atom_type='P'\n):\n    \"\"\"\n    Writes a single line of PDB format based on provided atom information.\n\n    Args:\n        atom_name (str): Name of the atom (e.g., \"N\", \"CA\").\n        atom_serial (int): Atom serial number.\n        residue_name (str): Residue name (e.g., \"ALA\").\n        chain_id (str): Chain identifier.\n        residue_num (int): Residue number.\n        x_coord (float): X coordinate.\n        y_coord (float): Y coordinate.\n        z_coord (float): Z coordinate.\n        occupancy (float, optional): Occupancy value (default: 1.0).\n        b_factor (float, optional): B-factor value (default: 0.0).\n\n    Returns:\n        str: A single line of PDB string.\n    \"\"\"\n    return f'ATOM  {atom_serial:>5d}  {atom_name:<5s} {residue_name:<3s} {residue_num:>3d}    {x_coord:>8.3f}{y_coord:>8.3f}{z_coord:>8.3f}{occupancy:>6.2f}{b_factor:>6.2f}           {atom_type}\\n'\n\ndef write_xyz_to_pdb(df, pdb_file, xyz_id = 1):\n    resolved_cnt = 0\n    with open(pdb_file, 'w') as target_file:\n        for _, row in df.iterrows():\n            x_coord = row[f'x_{xyz_id}']\n            y_coord = row[f'y_{xyz_id}']\n            z_coord = row[f'z_{xyz_id}']\n\n            if x_coord > -1e17 and y_coord > -1e17 and z_coord > -1e17:\n                resolved_cnt += 1\n                target_line = write_target_line(\n                    atom_name=\"C1'\",\n                    atom_serial=int(row['resid']),\n                    residue_name=row['resname'],\n                    chain_id='0',\n                    residue_num=int(row['resid']),\n                    x_coord=x_coord,\n                    y_coord=y_coord,\n                    z_coord=z_coord,\n                    atom_type='C',\n                )\n                target_file.write(target_line)\n    return resolved_cnt\n\ndef parse_usalign_for_tm_score(output):\n    # Extract TM-score based on length of reference structure (second)\n    tm_score_match = re.findall(r'TM-score=\\s+([\\d.]+)', output)[1]\n    if not tm_score_match:\n        raise ValueError('No TM score found')\n    return float(tm_score_match)\n\ndef parse_usalign_for_transform(output):\n    # Locate the rotation matrix section\n    matrix_lines = []\n    found_matrix = False\n\n    for line in output.splitlines():\n        if \"The rotation matrix to rotate Structure_1 to Structure_2\" in line:\n            found_matrix = True\n        elif found_matrix and re.match(r'^\\d+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+$', line):\n            matrix_lines.append(line)\n        elif found_matrix and not line.strip():\n            break  # Stop parsing if an empty line is encountered after the matrix\n\n    # Parse the rotation matrix values\n    rotation_matrix = []\n    for line in matrix_lines:\n        parts = line.split()\n        row_values = list(map(float, parts[1:]))  # Skip the first column (index)\n        rotation_matrix.append(row_values)\n\n    return np.array(rotation_matrix)\n\ndef call_usalign(predict_df, truth_df, verbose=1):\n    truth_pdb = '~truth.pdb'\n    predict_pdb = '~predict.pdb'\n    write_xyz_to_pdb(predict_df, predict_pdb, xyz_id=1)\n    write_xyz_to_pdb(truth_df, truth_pdb, xyz_id=1)\n\n    command = f'{USALIGN} {predict_pdb} {truth_pdb} -atom \" C1\\'\" -m -'\n    output = os.popen(command).read()\n    if verbose==1:\n        print(output)\n    tm_score = parse_usalign_for_tm_score(output)\n    transform = parse_usalign_for_transform(output)\n    return tm_score, transform\n\nprint('HELPER OK!!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:25.042164Z","iopub.execute_input":"2025-05-09T03:31:25.042404Z","iopub.status.idle":"2025-05-09T03:31:25.078895Z","shell.execute_reply.started":"2025-05-09T03:31:25.042380Z","shell.execute_reply":"2025-05-09T03:31:25.078009Z"}},"outputs":[{"name":"stdout","text":"PYTHON /usr/bin/python3\nHELPER OK!!!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"if MODEL_TYPE=='protenix':\n    \n    \n    from runner.batch_inference import get_default_runner\n    from runner.inference import update_inference_configs, InferenceRunner\n\n    from protenix.data.infer_data_pipeline import InferenceDataset\n\n    np.random.seed(0)\n    torch.random.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n\n    class DictDataset(InferenceDataset):\n        def __init__(\n            self,\n            seq_list: list,\n            dump_dir: str,\n            id_list: list = None,\n            use_msa: bool = False,\n        ) -> None:\n\n            self.dump_dir = dump_dir\n            self.use_msa = use_msa\n            if isinstance(id_list,type(None)):\n                self.inputs = [{\"sequences\": \n                                [{\"rnaSequence\": \n                                  {\"sequence\": seq, \n                                   \"count\": 1}}],\n                                \"name\": \"query\"} for seq in seq_list]\n            else:\n                self.inputs = [{\"sequences\": \n                                [{\"rnaSequence\": \n                                  {\"sequence\": seq, \n                                   \"count\": 1}}],\n                                \"name\": i} for i, seq in zip(id_list,seq_list)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:25.079608Z","iopub.execute_input":"2025-05-09T03:31:25.079834Z","iopub.status.idle":"2025-05-09T03:31:26.986795Z","shell.execute_reply.started":"2025-05-09T03:31:25.079795Z","shell.execute_reply":"2025-05-09T03:31:26.986163Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"if MODEL_TYPE=='protenix':\n\n    from configs.configs_base import configs as configs_base\n    from configs.configs_data import data_configs\n    from configs.configs_inference import inference_configs\n    from protenix.config.config import parse_configs\n\n    configs_base[\"use_deepspeed_evo_attention\"] = (\n    os.environ.get(\"USE_DEEPSPEED_EVO_ATTENTION\", False) == \"true\")\n    configs_base[\"model\"][\"N_cycle\"] = 10 #10\n    configs_base[\"sample_diffusion\"][\"N_sample\"] = (1 if VALIDATION else 5)\n    configs_base[\"sample_diffusion\"][\"N_step\"] = 200\n    inference_configs['load_checkpoint_path']='/kaggle/input/protenix-checkpoints/model_v0.2.0.pt'\n    configs = {**configs_base, **{\"data\": data_configs}, **inference_configs}\n\n    configs = parse_configs(\n            configs=configs,\n            fill_required_with_null=True,\n        )\n    \n    runner=InferenceRunner(configs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:26.987563Z","iopub.execute_input":"2025-05-09T03:31:26.987995Z","iopub.status.idle":"2025-05-09T03:31:43.408133Z","shell.execute_reply.started":"2025-05-09T03:31:26.987972Z","shell.execute_reply":"2025-05-09T03:31:43.407452Z"}},"outputs":[{"name":"stdout","text":"train scheduler 16.0\ninference scheduler 16.0\nDiffusion Module has 16.0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/runner/inference.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, self.device)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"if VALIDATION:\n    LABEL_DF = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_labels.csv')\n    LABEL_DF['target_id'] = LABEL_DF['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n    train_df=pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:43.410209Z","iopub.execute_input":"2025-05-09T03:31:43.410458Z","iopub.status.idle":"2025-05-09T03:31:43.754155Z","shell.execute_reply.started":"2025-05-09T03:31:43.410436Z","shell.execute_reply":"2025-05-09T03:31:43.753096Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\nif MODEL_TYPE=='protenix' and VALIDATION:\n    import warnings\n    warnings.filterwarnings(\"ignore\")  \n    \n    train_df['protenix_tm_score']=None\n    dataset = DictDataset(train_df.sequence, dump_dir='output', id_list=train_df.target_id, use_msa=False)\n    num_data = len(dataset)\n    for i, seq in tqdm(enumerate(train_df.sequence),total=num_data):\n        if train_df.loc[i,'protenix_tm_score']!=None:\n            continue\n        if len(seq)>300:\n            continue\n        target_id = train_df.loc[i,'target_id']\n        truth_df = get_truth_df(target_id)\n        if sum(~np.isnan(truth_df.x_1))<3:\n            continue\n        data, atom_array, data_error_message=dataset[i]\n        if data_error_message!='':\n            continue\n        new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n        runner.update_model_configs(new_configs)\n        prediction = runner.predict(data)\n        prediction=prediction['coordinate'][:,data['input_feature_dict']['atom_to_tokatom_idx']==12]       \n        result = parse_output_to_df(prediction[:1], seq, target_id)[0]\n        try:\n            tm_score, transform = call_usalign(result, truth_df, verbose=0)\n            train_df.loc[i,'protenix_tm_score']=tm_score\n        except:\n            pass\n        if (time.time()-time0)>(12*3600-360):\n            break\n    train_df.to_csv('tm_scores.csv', index=False)\n    print(train_df.protenix_tm_score.mean())\n    display(train_df.protenix_tm_score.hist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:33:55.977219Z","iopub.execute_input":"2025-05-09T03:33:55.977554Z","execution_failed":"2025-05-09T03:49:22.283Z"}},"outputs":[{"name":"stderr","text":"  8%|▊         | 66/844 [15:24<3:06:00, 14.35s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"if MODEL_TYPE=='protenix' and not VALIDATION:\n    test_df=pd.read_csv('/kaggle/input/stanford-rna-3d-folding/test_sequences.csv')\n    import warnings\n    warnings.filterwarnings(\"ignore\")  \n    \n    dataset = DictDataset(test_df.sequence, dump_dir='output', id_list=test_df.target_id, use_msa=False)\n    num_data = len(dataset)\n    for i, seq in tqdm(enumerate(test_df.sequence),total=num_data):\n        try:\n            data, atom_array, data_error_message=dataset[i]\n            target_id = data[\"sample_name\"]\n            assert target_id==test_df.target_id[i]\n            assert data_error_message==''\n            \n            new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n            runner.update_model_configs(new_configs)\n            prediction = runner.predict(data)\n            prediction=prediction['coordinate'][:,data['input_feature_dict']['atom_to_tokatom_idx']==12]\n\n            result = parse_output_to_df(prediction, seq, target_id)[0]\n        except:\n            target_id==test_df.target_id[i]\n            print('Failed to predict', target_id)\n            result=pd.DataFrame(columns=['ID', 'resname', 'resid', \n                                         'x_1', 'y_1', 'z_1', \n                                         'x_2', 'y_2', 'z_2',\n                                         'x_3', 'y_3', 'z_3', \n                                         'x_4', 'y_4', 'z_4', \n                                         'x_5', 'y_5', 'z_5'], \n                                         data=[[target_id, x, j+1] + [0.0]*15 for j, x in enumerate(seq)])\n            \n        result['ID']=result.apply(lambda x: x.ID + '_' + str(x.resid), axis=1)\n        result.to_csv('submission.csv', index=False, mode='a', header=(i==0))\n        torch.cuda.empty_cache()\n\n    display(pd.read_csv('submission.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:31:43.764420Z","iopub.status.idle":"2025-05-09T03:31:43.764684Z","shell.execute_reply":"2025-05-09T03:31:43.764579Z"}},"outputs":[],"execution_count":null}]}